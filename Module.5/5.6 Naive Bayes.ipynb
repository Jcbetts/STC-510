{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "42231fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from string import punctuation\n",
    "from datetime import datetime\n",
    "\n",
    "DIRNAME = 'blogs/'\n",
    "\n",
    "english_stopwords = set(stopwords.words('english') + list(punctuation) + \n",
    "                       ['...', '..', '....', '``', \"''\", '/n'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a98f779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_wordlist(filelist):\n",
    "    '''\n",
    "    Takes a list of filenames with XML code, opens these and provides \n",
    "    a word list for all the posts, lemmatized without stopwords.\n",
    "    '''\n",
    "    corpus = ''\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for eachfile in filelist:\n",
    "        xmltext = open(eachfile, encoding = 'utf8', errors = 'replace').read()\n",
    "        soup = BeautifulSoup(xmltext, features = 'xml')\n",
    "        corpus += '/n'.join([x.text.lower() for x in soup.findAll('post')])\n",
    "    wordlist = [lemmatizer.lemmatize(x) for x in word_tokenize(corpus)\n",
    "               if x not in english_stopwords]\n",
    "    return wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "055a4a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "blogfiles = [ DIRNAME + fn for fn in os.listdir(DIRNAME) if not fn.startswith('.')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ebc3ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "blogteens = [ 1 if re.search(r'\\.1[3-9]\\.', x) else 0 for x in blogfiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ffd9983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('blogs/3699726.female.14.indUnk.Aries.xml', 1),\n",
       " ('blogs/3362256.male.15.indUnk.Scorpio.xml', 1),\n",
       " ('blogs/4270277.male.23.indUnk.Aquarius.xml', 0),\n",
       " ('blogs/3473863.female.26.Internet.Capricorn.xml', 0),\n",
       " ('blogs/2889926.female.25.Telecommunications.Cancer.xml', 0),\n",
       " ('blogs/3897591.male.15.Student.Libra.xml', 1),\n",
       " ('blogs/4216005.male.23.Consulting.Taurus.xml', 0),\n",
       " ('blogs/4251559.female.23.Non-Profit.Gemini.xml', 0),\n",
       " ('blogs/3584627.female.17.Student.Scorpio.xml', 1),\n",
       " ('blogs/4292654.female.33.Banking.Libra.xml', 0),\n",
       " ('blogs/4268277.female.14.indUnk.Leo.xml', 1),\n",
       " ('blogs/4176187.female.13.indUnk.Aquarius.xml', 1),\n",
       " ('blogs/3837608.male.24.Communications-Media.Sagittarius.xml', 0),\n",
       " ('blogs/3818017.male.15.indUnk.Aries.xml', 1),\n",
       " ('blogs/3476502.female.16.Education.Aries.xml', 1),\n",
       " ('blogs/3840468.male.23.indUnk.Gemini.xml', 0),\n",
       " ('blogs/3321827.male.38.Technology.Sagittarius.xml', 0),\n",
       " ('blogs/3877768.male.25.Technology.Libra.xml', 0),\n",
       " ('blogs/4298893.female.25.Consulting.Libra.xml', 0),\n",
       " ('blogs/4285714.female.35.Student.Capricorn.xml', 0),\n",
       " ('blogs/3319203.male.15.indUnk.Pisces.xml', 1),\n",
       " ('blogs/4224119.male.14.indUnk.Aquarius.xml', 1),\n",
       " ('blogs/3699630.male.16.Student.Aries.xml', 1),\n",
       " ('blogs/3352064.male.37.Religion.Scorpio.xml', 0),\n",
       " ('blogs/3480793.male.27.Law.Sagittarius.xml', 0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(blogfiles,blogteens))[:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d346eaf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:21:44\n",
      "13:30:00\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now().strftime('%H:%M:%S'))\n",
    "allblogs = [ ' '.join(clean_wordlist([x])) for x in blogfiles ]\n",
    "print(datetime.now().strftime('%H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b85e78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'filename':blogfiles,\n",
    "                  'teen':blogteens,\n",
    "                   'text':allblogs})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "08c7d551",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(df.text, df.teen,\n",
    "                                                   random_state=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9a47ee",
   "metadata": {},
   "source": [
    "X here is the text and Y here is the whether it is a teen blog\n",
    "Can X determine Y?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e7a24e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorize = TfidfVectorizer(use_idf=True)\n",
    "X_train_tf = tfidf_vectorize.fit_transform(X_train)\n",
    "X_test_tf = tfidf_vectorize.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "380c4757",
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(X_train_tf, Y_train)\n",
    "predictions = naive_bayes.predict(X_test_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f211b151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6575569358178054\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: ', accuracy_score(Y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08469a7",
   "metadata": {},
   "source": [
    "Accuracy still around 65% when using the entire dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
